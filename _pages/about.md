---
permalink: /
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I received my Ph. D on [Faster, Incentivized, and Efficient Federated Learning: Theory and Applications](https://www.proquest.com/openview/b4f75e9c959676ff3655114c1be31ac1/1?pq-origsite=gscholar&cbl=18750&diss=y) at Carnegie Mellon Univ. on Aug. 2024, and joined Google to work on On-device Machine Learning for LLMs. My research interests are in distributed machine learning, on-device machine learning, and federated learning.



Scroll down to see more.

News
------
* Sep. 2024: Our work in collaboration with Google Research on [Heterogeneous LoRA for Federated Fine-tuning of LLMs](https://arxiv.org/abs/2401.06432) has been accepted to EMNLP main proc.! See you at Miami! 
* Aug. 2024: Defended my Thesis on [Faster, Incentivized, and Efficient Federated Learning: Theory and Applications](https://www.proquest.com/openview/b4f75e9c959676ff3655114c1be31ac1/1?pq-origsite=gscholar&cbl=18750&diss=y)!
* Jul. 2024: Joined Google to work on On-device Machine Learning for LLMs
* Dec. 2023: Our work done during my summer at Google Research on [parameter-efficient federated fine-tuning with heterogeneous Low-Rank Approximation](https://openreview.net/forum?id=EmV9sGpZ7q) has been accepted to FL@FM-NeurIPS'23! See you at New Orleans!
* Jul. 2023: Our work done during my summer at Microsoft Research on [federated learning with limited labels](https://arxiv.org/pdf/2307.08809.pdf) is accepted to ICCV 2023! See you at Paris!
* May. 2023: I will be interning at Google Research, Seattle working on the intersection of LLMs and FL this summer!
* Apr. 2023: Our work on [cyclic client participation in federated learning](https://arxiv.org/abs/2302.03109) is accepted to ICML 2023 (28% acceptance rate)! See you at Honolulu!
* Nov. 2022: I am invited to the Women in Research Lean In 2022 event hosted by Meta! See you at Menlo Park!
* Oct. 2022: Our work on [client incentives in federated learning](https://arxiv.org/pdf/2205.14840.pdf) is accepted to [FL-NeurIPS'22](https://federated-learning.org/fl-neurips-2022/) for oral presentation (12% acceptance rate)! See you at New Orleans!
* Aug. 2022: I finished my second summer research internship at Microsoft Research's Privacy in AI Team working on semi-supervised federated learning!
* Apr. 2022: Our team was selected as the Finalist for the [2022 Qualcomm Innovation Fellowship](https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-north-america) for research on Incentivized Federated Learning for Data-Heterogeneous and Resource-Constrained Clients
* Mar. 2022: I gave a talk at the [MLOPT Research Group's Idea Seminar at UW-Madison](https://mlopt.ece.wisc.edu/idea-seminar/) on [Leveraging Biased Client Selection in Federated Learning](https://www.pdl.cmu.edu/PDL-FTP/associated/2010.01243.pdf) -- work accepted to AISTATS 2022 (29% acceptance rate)
* Aug. 2021: I finished my summer research internship at Microsoft Research's Privacy in AI Team working on [Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning](https://arxiv.org/pdf/2204.12703.pdf) -- work accepted to IJCAI 2022 (15% acceptance rate)


Selected Publications
------
* <ins>Y. J. Cho</ins>, L. Liu, Z. Xu, A. Fahrezi, and G. Joshi, "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models", EMNLP 2024 [[pdf]](https://arxiv.org/abs/2401.06432)
* <ins>Y. J. Cho</ins>, L. Liu, Z. Xu, A. Fahrezi, M. Barnes, and G. Joshi, "Heterogeneous LoRA for Federated Fine-tuning of On-device Foundation Models", FL@FM-NeurIPS'23 [[pdf]](https://openreview.net/pdf?id=EmV9sGpZ7q)
* <ins>Y. J. Cho</ins>, G. Joshi, and D. Dimitriadis, "Local or Global: Selective Knowledge Assimilation
for Federated Learning with Limited Labels", ICCV 2023 [[pdf]](https://arxiv.org/abs/2307.08809)
* <ins>Y. J. Cho</ins>, P. Sharma, G. Joshi, Z. Xu, S. Kale, and T. Zhang, "On the Convergence of Federated Averaging with Cyclic Client Participation", ICML 2023 [[pdf]](https://arxiv.org/abs/2302.03109)
* <ins>Y. J. Cho</ins>, D. Jhunjhunwala, T. Li, V. Smith, and G. Joshi, "To Federate or Not To Federate: Incentivizing Client Participation in Federated Learning", TMLR 2024, Shorter version at FL-NeurIPS'22 (oral) [[pdf]](https://arxiv.org/abs/2205.14840)
* <ins>Y. J. Cho</ins>, J. Wang, T. Chiruvolu, and G. Joshi, "Personalized Federated Learning for Heterogeneous Devices with Clustered Knowledge Transfer", <em>IEEE Journal of Selected Topics in Signal Processing (IEEE JSTSP), Dec 2022 </em> [[pdf]](https://arxiv.org/pdf/2109.08119.pdf)
* <ins>Y. J. Cho</ins>, A. Manoel, G. Joshi, R. Sim, and D. Dimitriadis, "Heterogeneous Ensemble Knowledge Transfer for Training Large Models in
Federated Learning", <em>IJCAI 2022</em> [[pdf]](https://arxiv.org/pdf/2204.12703.pdf)
* <ins>Y. J. Cho</ins>, J. Wang, and G. Joshi, "Client Selection in Federated Learning: Convergence
Analysis and Power-of-Choice Selection Strategies", <em>AISTATS 2022</em> [[pdf]](https://arxiv.org/pdf/2010.01243.pdf)
* <ins>Y. J. Cho</ins>, S. Gupta, G. Joshi, and O. Yagan, "Bandit-based Communication-Efficient Client
Selection Strategies for Federated Learning", <em>Asilomar Conference on Signals, Systems and Computers 2020 (Invited Paper)</em> [[pdf]](https://arxiv.org/pdf/2012.08009.pdf)





Miscellaneous
------
In my free time I enjoy playing the piano, squash, swimming, and spending time with <font face="monospace" color="#1e90ff">PanitheCorgi</font>.

<img src="/images/panipic.jpg" width="30%">
<p><font face="monospace" color="#1e90ff">PanitheCorgi</font> loves <span>&#10052;</span> </p> 


<br>


<p style="color:Grey; font-size: 15px;"> Last updated on Jan. 2024.</p>

